{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kili Tutorial: AutoML for faster labeling with Kili Technology\n",
    "In this tutorial, we will show how to use [automated machine learning](https://en.wikipedia.org/wiki/Automated_machine_learning) (AutoML) to accelerate labeling in Kili Technology. We will apply it in the context of text classification: given a tweet, I want to classify whether it is about a real disaster or not (as introduced in [Kaggle NLP starter kit](https://www.kaggle.com/c/nlp-getting-started)).\n",
    "\n",
    "Why want to label more data when Kaggle often provides with a fully annotated training set and a testing set?\n",
    "\n",
    "- Annotate the testing set in order to have more training data once you fine-tuned an algorithm (once you are sure you do not overfit). More data almost always means better scores in machine learning.\n",
    "- As a data scientist, annotate data in order to get a feel of what data looks like and what ambiguities are.\n",
    "\n",
    "But annotating data is a time-consuming task. So we would like to help you annotate faster by fully automating machine learning models thanks to AutoML. Here is what is looks like in Kili:\n",
    "\n",
    "<img src=\"./img/automl.gif\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Additionally:\n",
    "\n",
    "For an overview of Kili, visit [kili-technology.com](https://kili-technology.com). You can also check out [Kili documentation](https://cloud.kili-technology.com/docs).\n",
    "\n",
    "The tutorial is divided into three parts:\n",
    "\n",
    "1. AutoML\n",
    "2. Integrate AutoML scikit-learn pipelines\n",
    "3. Automating labeling in Kili Technology\n",
    "\n",
    "## 1. AutoML\n",
    "Automated machine learning (AutoML) is described as the process of automating both the choice and training of a machine learning algorithm by automatically optimizing its hyperparameters.\n",
    "\n",
    "There already exist many AutoML framework:\n",
    "\n",
    "- [H2O](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html) provides with an AutoML solution with both Python and R bindings\n",
    "- [autosklearn](https://automl.github.io/auto-sklearn/master/) can be used for SKLearn pipelines\n",
    "- [TPOT](http://epistasislab.github.io/tpot) uses genetic algorithms to automatically tune your algorithms\n",
    "- [fasttext](https://fasttext.cc) has [its own AutoML module](https://fasttext.cc/docs/en/autotune.html) to find the best hyperparameters\n",
    "\n",
    "We will cover the use of `autosklearn` for automated text classification. `autosklearn` explores the hyperparameters grid as defined by SKLearn as a human would [do it manually](https://scikit-learn.org/stable/modules/grid_search.html). Jobs can be run in parallel in order to speed up the exploration process. `autosklearn` can use either [SMAC](http://ml.informatik.uni-freiburg.de/papers/11-LION5-SMAC.pdf) (Sequential Model-based Algorithm Configuration) or [random search](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) to select the next set of hyperparameters to test at each time.\n",
    "\n",
    "Once AutoML automatically chose and trained a classifier, we can use this classifier to make predictions. Predictions can then be inserted into Kili Technology. When labeling, labelers first see predictions before labeling. For complex tasks, this can considerably speed up the labeling.\n",
    "\n",
    "For instance, when annotating voice for [automatic speech recognition](https://en.wikipedia.org/wiki/Speech_recognition), if you use a model that pre-annotates by transcribing speeches, you more than double annotation productivity:\n",
    "\n",
    "<img src=\"./img/efficiency_comparison_with_without_model.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Integrate AutoML scikit-learn pipelines\n",
    "\n",
    "Specifically for text classification, the following pipeline retrieves labeled and unlabeled data from Kili, builds a classifier using AutoML and then enriches back Kili's training set:\n",
    "\n",
    "<img src=\"./img/automl_pipeline.png\" alt=\"Drawing\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After retrieving data, [TFIDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) pre-processes text data by filtering out common words (such as `the`, `a`, etc) in order to make most important features stand out. These pre-processed features will be fed to a classifier.\n",
    "\n",
    "*Note:* `autosklearn` runs [better](https://automl.github.io/auto-sklearn/master/installation.html#windows-osx-compatibility) on Linux, so we recommand running code snippets inside a [Docker image](https://hub.docker.com/r/mfeurer/auto-sklearn/):\n",
    "\n",
    "```\n",
    "docker run --rm -it -p 10000:8888 -v /local/path/to/notebook/folder:/home/kili --entrypoint \"/bin/bash\" mfeurer/auto-sklearn\n",
    "# cd /home/kili && jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "MIN_DOC_FREQ = 2\n",
    "NGRAM_RANGE = (1, 2)\n",
    "TOP_K = 20000\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    tfidf_vectorizer_params = {\n",
    "        'ngram_range': NGRAM_RANGE,\n",
    "        'dtype': 'int32',\n",
    "        'strip_accents': 'unicode',\n",
    "        'decode_error': 'replace',\n",
    "        'analyzer': TOKEN_MODE,\n",
    "        'min_df': MIN_DOC_FREQ,\n",
    "    }\n",
    "\n",
    "    # Learn vocab from train texts and vectorize train and val sets\n",
    "    tfidf_vectorizer = TfidfVectorizer(**tfidf_vectorizer_params)\n",
    "    x_train = tfidf_vectorizer.fit_transform(train_texts)\n",
    "    x_val = tfidf_vectorizer.transform(val_texts)\n",
    "\n",
    "    # Select k best features, with feature importance measured by f_classif\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "\n",
    "    return x_train, x_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeled data is split in train and test sets for validation. Then, `autosklearn` classifier is chosen and trained in a limited time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "# Un comment these lines if you are not running inside autosklearn container\n",
    "# !conda install gxx_linux-64 gcc_linux-64 swig==3.0.12 --yes\n",
    "# !pip install auto-sklearn\n",
    "\n",
    "import autosklearn\n",
    "import autosklearn.classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def automl_train_and_predict(X, y, X_to_predict):\n",
    "    x, x_to_predict = ngram_vectorize(\n",
    "        X, y, X_to_predict)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Auto-tuning by autosklearn\n",
    "    cls = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=200,\n",
    "                                                           per_run_time_limit=20,\n",
    "                                                           seed=10)\n",
    "    cls.fit(x_train, y_train)\n",
    "    assert x_train.shape[1] == x_to_predict.shape[1]\n",
    "\n",
    "    # Performance metric\n",
    "    predictions_test = cls.predict(x_test)\n",
    "    print('Accuracy: {}'.format(accuracy_score(y_test, predictions_test)))\n",
    "\n",
    "    # Generate predictions\n",
    "    predictions = cls.predict(x_to_predict)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automating labeling in Kili Technology\n",
    "Let's now feed Kili data to the AutoML pipeline. For that you will need to [create a new](https://cloud.kili-technology.com/label/projects/create-project) `Text classification` project. Assets are taken from Kaggle challenge `Real or Not? NLP with Disaster Tweets`. You can download them [here](https://www.kaggle.com/c/nlp-getting-started/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Kili Technology using `kili-playground` (Kili's official [Python SDK](https://github.com/kili-technology/kili-playground) to interact with Kili API):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kili\n",
    "from kili.client import Kili\n",
    "\n",
    "api_key = 'YOUR API KEY'\n",
    "project_id = 'YOUR PROJECT ID'\n",
    "api_endpoint = 'https://cloud.kili-technology.com/api/label/v2/graphql'\n",
    "\n",
    "kili = Kili(api_key=api_key, api_endpoint=api_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's insert all assets into Kili. You can download the original unannotated `test.csv` directly [on Kaggle](https://www.kaggle.com/c/nlp-getting-started/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./datasets/test.csv')\n",
    "content_array = []\n",
    "external_id_array = []\n",
    "for index, row in df.iterrows():\n",
    "    external_id_array.append(f'tweet_{index}')\n",
    "    content_array.append(row['text'])\n",
    "\n",
    "kili.append_many_to_dataset(project_id=project_id,\n",
    "                                  content_array=content_array,\n",
    "                                  external_id_array=external_id_array,\n",
    "                                  is_honeypot_array=[False for _ in content_array],\n",
    "                                  status_array=['TODO' for _ in content_array],\n",
    "                                  json_metadata_array=[{} for asset in content_array])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the categories of the first job that you defined in Kili interface. Learn [here](https://cloud.kili-technology.com/docs/projects/customize-interface/) what interfaces and jobs are in Kili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = kili.projects(project_id=project_id)[0]\n",
    "assert 'jsonInterface' in project\n",
    "\n",
    "json_interface = project['jsonInterface']\n",
    "jobs = json_interface['jobs']\n",
    "jobs_list = list(jobs.keys())\n",
    "assert len(jobs_list) == 1, 'More than one job was defined in the interface'\n",
    "\n",
    "job_name = jobs_list[0]\n",
    "job = jobs[job_name]\n",
    "categories = list(job['content']['categories'].keys())\n",
    "print(f'Categories are: {categories}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continuously fetch assets from Kili Technology and apply AutoML pipeline. You can launch the next cell and go to Kili in order to label. After labeling a few assets, you'll see predictions automatically pop up in Kili!\n",
    "\n",
    "Go [here](https://github.com/kili-technology/kili-playground/blob/master/recipes/import_predictions.ipynb) to learn in more details how to insert predictions into Kili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SECONDS_BETWEEN_TRAININGS = 60\n",
    "\n",
    "def extract_train_for_auto_ml(job_name, assets, categories, train_test_threshold=0.8):\n",
    "    X = []\n",
    "    y = []\n",
    "    X_to_predict = []\n",
    "    ids_X_to_predict = []\n",
    "    for asset in assets:\n",
    "        x = asset['content']\n",
    "        labels = [l for l in asset['labels'] if l['labelType'] in ['DEFAULT', 'REVIEWED']]\n",
    "\n",
    "        # If no label, add it to X_to_predict\n",
    "        if len(labels) == 0:\n",
    "            X_to_predict.append(x)\n",
    "            ids_X_to_predict.append(asset['externalId'])\n",
    "\n",
    "        # Otherwise add it to training examples X, y\n",
    "        for label in labels:\n",
    "            jsonResponse = label['jsonResponse'][job_name]\n",
    "            is_empty_label = 'categories' not in jsonResponse or len(\n",
    "                jsonResponse['categories']) != 1 or 'name' not in jsonResponse['categories'][0]\n",
    "            if is_empty_label:\n",
    "                continue\n",
    "            X.append(x)\n",
    "            y.append(categories.index(\n",
    "                jsonResponse['categories'][0]['name']))\n",
    "    return X, y, X_to_predict, ids_X_to_predict\n",
    "\n",
    "while True:\n",
    "    print('Export assets and labels...')\n",
    "    assets = kili.assets(project_id=project_id, first=100, skip=0) ## Remove that\n",
    "    X, y, X_to_predict, ids_X_to_predict = extract_train_for_auto_ml(job_name, assets, categories)\n",
    "\n",
    "    version = 0\n",
    "    if len(X) > 5:\n",
    "        print('AutoML is on its way...')\n",
    "        predictions = automl_train_and_predict(X, y, X_to_predict)\n",
    "\n",
    "        print('Inserting predictions to Kili...')\n",
    "        external_id_array = []\n",
    "        json_response_array = []\n",
    "        for i, prediction in enumerate(tqdm(predictions)):\n",
    "            json_response = {\n",
    "                job_name: {\n",
    "                    'categories': [{\n",
    "                        'name': categories[prediction],\n",
    "                        'confidence':100\n",
    "                    }]\n",
    "                }\n",
    "            }\n",
    "            external_id_array.append(ids_X_to_predict[i])\n",
    "            json_response_array.append(json_response)\n",
    "\n",
    "        # Good practice: version your model so you know the result of every model\n",
    "        kilipla.create_predictions(project_id=project_id,\n",
    "                                      external_id_array=external_id_array,\n",
    "                                      model_name_array=[f'automl-{version}']*len(external_id_array),\n",
    "                                      json_response_array=json_response_array)\n",
    "        print('Done.\\n')\n",
    "    time.sleep(SECONDS_BETWEEN_TRAININGS)\n",
    "    version += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "In this tutorial, we accomplished the following:\n",
    "\n",
    "We introduced the concept of AutoML as well as several of the most-used frameworks for AutoML. We demonstrated how to leverage AutoML to automatically create predictions in Kili. If you enjoyed this tutorial, check out the other Recipes for other tutorials that you may find interesting, including demonstrations of how to use Kili.\n",
    "\n",
    "You can also visit the Kili website or Kili documentation for more info!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
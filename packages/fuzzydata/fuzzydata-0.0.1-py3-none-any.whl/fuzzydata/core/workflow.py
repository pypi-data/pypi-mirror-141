from __future__ import annotations

import glob
import os
import logging
import json
import time

from abc import ABC, abstractmethod
from typing import Dict, List

import networkx as nx
import numpy as np
import pandas as pd

from fuzzydata.core.artifact import Artifact
from fuzzydata.core.generator import generate_schema
from fuzzydata.core.operation import Operation


logger = logging.getLogger(__name__)


class Workflow(ABC):
    """
    Class to represent a workflow in fuzzydata, Extends DiGraph from networkx with additional metadata about
    the workflow as required.
    """
    def __init__(self, name='wf', out_directory='/tmp/fuzzydata/wf/'):
        """
        Create a new workflow with a specified name
        :param name: Name of the workflow
        :param out_directory: Output Directory for this workflow
        """

        self.name = name
        self.graph = nx.DiGraph()
        self.out_dir = out_directory
        self.artifact_dir = f"{self.out_dir}/artifacts/"
        os.makedirs(self.artifact_dir, exist_ok=True)
        self.artifact_list = []
        self.artifact_dict = {}

        self.artifact_class = None
        self.operator_class = None

        self.operation_list = []

        self.perf_records = []
        #self.perf_df = pd.DataFrame()

        logger.info(f'Creating new Workflow {self.name}')

    def generate_next_label(self):
        return f"artifact_{len(self)}"

    @abstractmethod
    def initialize_new_artifact(self, label=None, filename=None, schema_map=None) -> Artifact:
        pass

    def add_artifact(self, artifact: Artifact,
                     from_artifacts: List[Artifact] = None, operation: Operation = None) -> None:
        """
        Add a new artifact to the workflow with label and dataframe.
        Optionally add source label and edge information
        :param artifact: The artifact to be added
        :param from_artifacts: (optional) Source artifacts from which this new artifact was derived
        :param operation: Operation used to derive this new artifact
        """
        self.graph.add_node(artifact.label,
                            **{
                                'schema_map': artifact.schema_map,
                                'file_format': artifact.file_format,
                                'filename': artifact.filename
                            })
        v = artifact.label

        self.artifact_list.append(artifact.label)
        self.artifact_dict[artifact.label] = artifact

        if from_artifacts:
            for u in from_artifacts:
                self.graph.add_edge(u.label, v, **{
                    'operation': operation.op.__name__,
                    'args': operation.args
                })

    def generate_base_artifact(self, num_rows=100, num_cols=10, column_maps=None, label: str = None) -> Artifact:
        """
        Create a base artifact of with given rows and columns
        :param num_rows: number of rows to be generated
        :param num_cols: number of columns to be generated
        :param column_maps: (optional) schema map for the table to be generated
        :param label: (optional) custom label for the artifact to be generated
        :return:
        """
        if not column_maps:
            column_maps = generate_schema(num_cols)
        if not label:
            label = self.generate_next_label()
        start_time = time.perf_counter()
        new_artifact = self.initialize_new_artifact(label=label, filename=f"{self.artifact_dir}/{label}.csv",
                                                    schema_map=column_maps)
        new_artifact.generate(num_rows, column_maps)
        end_time = time.perf_counter()
        self.add_artifact(new_artifact)

        self.perf_records.append(pd.Series({
            'src': np.nan,
            'dst': label,
            'op': 'generate',
            'args': np.nan,
            'start_time': start_time,
            'end_time': end_time,
            'elapsed_time': end_time - start_time
        }).to_frame().T)

        return new_artifact

    def generate_artifact_from_operation(self, artifacts: List[Artifact], op: str,
                                         args: Dict, new_label: str = None) -> Artifact:
        """
        Generate a new artifact with source artifacts apply operation op with args parameters.
        :param new_label: Optional label annotation for the artifact to be generated. Autogenerated if not specified.
        :param artifacts: list of source artifacts for the operation to be applied.
        :param op: string label of the operation to be applied.
        :param args: dict containing all the parameters to be passed to the operation function.
        :return:
        """
        if not new_label:
            new_label = self.generate_next_label()

        operation = self.operator_class(sources=artifacts, new_label=new_label, op=op, args=args,
                                        artifact_class=self.artifact_class)
        try:
            new_artifact = operation.execute()
            self.add_artifact(new_artifact, from_artifacts=artifacts, operation=operation)

            # TODO: Exception Handling and return value on op failure / empty df

            # Add operation to op list
            self.operation_list.append(operation.to_dict())

            # Add performance information
            self.perf_records.append(pd.Series({
                'src': tuple(x.label for x in artifacts),
                'dst': new_label,
                'op': op,
                'args': args,
                'start_time': operation.start_time,
                'end_time': operation.end_time,
                'elapsed_time': operation.get_execution_time()
            }).to_frame().T)

        except ValueError as e:
            logger.error(f'Could not execute Operation: {op} with args {args}')
            op_dict = operation.to_dict()
            op_dict['status'] = 'error'
            self.operation_list.append(op_dict)
            self.serialize_workflow()
            raise e

        return new_artifact

    def serialize_workflow(self, output_dir: str = None) -> None:
        if not output_dir:
            output_dir = self.out_dir

        # Create Output Directories
        artifact_dir = f"{output_dir}/artifacts/"
        os.makedirs(artifact_dir, exist_ok=True)

        # Write out all artifacts
        for label, artifact in self.artifact_dict.items():
            logger.debug(f"Serialization {label}, {artifact.label}")
            artifact.serialize(filename=f"{artifact_dir}/{label}.{artifact.file_format}")

        # Write out Operation List JSON
        with open(f"{output_dir}/{self.name}_operations.json", 'w') as outfile:
            outfile.write(json.dumps({'name': self.name,
                                      'operation_list': [op for op in self.operation_list]
                                      }, indent=2))

        # Write out Lineage Graph
        nx.write_edgelist(self.graph, f"{output_dir}/{self.name}_gt_graph.csv")

        # Construct Schema Map dict and write out as json
        schema_map_dict = {label: artifact.schema_map for label, artifact in self.artifact_dict.items()}
        with open(f"{output_dir}/{self.name}_schema_map.json", 'w') as outfile:
            outfile.write(json.dumps(schema_map_dict, indent=2))

        # Write out performance table
        self.write_perf()

    @classmethod
    def load_workflow(cls, input_dir: str, out_directory: str, name=None, replay=False,
                      wf_options={}, scale_artifact={}) -> Workflow:
        try:
            artifact_dir = f"{input_dir}/artifacts/"
            operations_file = glob.glob(f"{input_dir}/*_operations.json")[0]

            with open(operations_file, 'r') as infile:
                ops = json.load(infile)

            if name is None:
                name = ops['name']

            workflow = cls(name=name, out_directory=out_directory, **wf_options)

            if replay:
                schema_map_file = glob.glob(f"{input_dir}/*_schema_map.json")[0]
                with open(schema_map_file, 'r') as infile:
                    all_schema_maps = json.load(infile)
                workflow.replay_op_list(artifact_dir, op_list=ops['operation_list'], all_schema_maps=all_schema_maps,
                                        scale_artifact=scale_artifact)
                workflow.write_perf()

            # Revisit copying the workflow graph over, currently replay does this for us.
            # workflow.graph = nx.read_edgelist(f"{input_dir}/{workflow.name}_gt_graph.csv")

            return workflow

        except FileNotFoundError as e:
            logger.error(f"Error Loading Workflow from {input_dir}: {e}")

    def replay_op_list(self, artifact_dir: str, op_list=None, all_schema_maps=None, scale_artifact={}) -> None:
        for op in op_list:
            for source in op['sources']:
                if source not in self.artifact_dict.keys():
                    # TODO: Handle PK-FK merges properly - if DF is merge input, we need to maintain the keyspace and
                    # column schema maybe?
                    if source in scale_artifact.keys():
                        logger.info(f"Scaling up Artifact {source} to size {scale_artifact[source]}")
                        source_artifact = self.generate_base_artifact(num_rows=scale_artifact[source],
                                                                      label=source,
                                                                      column_maps=all_schema_maps[source])
                    else:
                        logger.info(f"Loading Pre-Generated Artifact: {source} ")
                        source_artifact = self.initialize_new_artifact(label=source, schema_map=all_schema_maps[source])
                        start_time = time.perf_counter()
                        source_artifact.deserialize(filename=f"{artifact_dir}/{source}.{source_artifact.file_format}")
                        end_time = time.perf_counter()
                        self.perf_records.append(pd.Series({
                            'src': source,
                            'dst': np.nan,
                            'op': 'load',
                            'args': np.nan,
                            'start_time': start_time,
                            'end_time': end_time,
                            'elapsed_time': end_time - start_time
                        }).to_frame().T)
                        self.add_artifact(source_artifact)

            logger.info(f"Replaying Operation: {tuple(a for a in op['sources'])} "
                        f"=={op['op']}==> {op['new_label']}")
            self.generate_artifact_from_operation([self.artifact_dict[x] for x in op['sources']],
                                                  op['op'], op['args'], new_label=op['new_label'])

    def write_perf(self, filename=None):
        if not filename:
            filename = f"{self.out_dir}/{self.name}_perf.csv"

        if self.perf_records:
            pd.concat(self.perf_records, ignore_index=True).to_csv(filename)
        else:
            logger.warning('No Performance Data to be Written')

    def select_random_artifact(self, bfactor=0.5, exclude: List[str] = None) -> Artifact:
        viable_artifacts = dict(filter(lambda x: x[0] not in exclude, self.artifact_dict.items()))
        size = len(viable_artifacts)
        a = np.arange(size)  # an array of the index value for weighting
        prob = np.exp(a/bfactor)  # higher weights for larger index values

        a = np.arange(size)
        prob = (bfactor / (np.exp(bfactor*(size))-1)) * np.exp(bfactor*a)
        prob = prob / prob.sum()

        return self.artifact_dict[np.random.choice(list(viable_artifacts.keys()), 1, p=prob)[0]]

    def __len__(self):
        return len(self.artifact_list)

    def __getitem__(self, item):
        return self.artifact_dict[item]

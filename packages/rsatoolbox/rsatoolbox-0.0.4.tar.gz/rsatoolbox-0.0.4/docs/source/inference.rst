.. _inference:

Inference
=========
Inference is the step of evaluating a set of models on a set of data RDMs and estimating our statistical uncertainty about these evaluations.

We distinguish three tiers of inference complexity. The first covers generalization over subjects only. The second adds generalization over
conditions. And the third enables the handling of fitted models by adding cross-validation.

All inference methods are implemented in ``rsatoolbox.inference``. They are named ``eval_*`` and take a list of :ref:`models <model>`
and a :ref:`RDMs object<distances>` of data RDMs as primary inputs and return a results object. Additionally, the string input `method`
allows you to specify which :ref:`RDM comparison measure<comparing>` to use.
This results object contains all information required for further tests including the (co-)variance estimates
for model evaluations and potentially the bootstrap samples computed.

All examples on this page assume that your models are saved in a list of :ref:`model <model>` called ``models`` and your measured data RDMs
are saved as an :ref:`RDMs object<distances>` ``rdms``.
A plot of the results can be generated by using ``rsatoolbox.vis.plot_model_comparison``, which is explained further :ref:`here<model plot>`.

A more interactive introduction to this topic is given by this :doc:`Demo <demo_bootstrap>`.

1. Basic inference
------------------
The fastest form inference evaluates a set of models with fixed predictions and estimates the uncertainty based on the variability across
data RDMs. This warrants only generalization to new subjects, not to conditions. Thus, it is formally only applicable to situations where
the used conditions cover the whole set of conditions we are interested in. An example would be RDMs across movements of the 5 fingers.
In this situation, there are no other conditions we want to generalize to.

For this type of inference use ``rsatoolbox.inference.eval_fixed``. This method will evaluate all models and estimate the variances based on
the variance across data RDMs.

Example:

.. code-block:: python

    results = rsatoolbox.inference.eval_fixed(models, rdms, method='corr_cov')
    rsatoolbox.vis.plot_model_comparison(results)

This function takes one additional argument: ``theta``. This argument allows you to set a fixed parameter for flexible models meant to enter this
type of evaluation. It should then be a list of numpy array parameter values in the same order as the models.

The variance caused by random sampling of the subjects can also be estimated by using bootstrapping.
This is implemented in ``rsatoolbox.inference.eval_bootstrap_rdm``. In expectation the variance computed by this method is the same as the one
computed by ``eval_fixed``. For this type of analysis it is thus not recommended to use bootstrapping.


2. Generalization over conditions
---------------------------------
There is no direct formula for the variance caused by random sampling of the conditions. Thus, we resort to bootstrapping to estimate this variance.

If we want to generalize only to the population of conditions for the exact subjects measured we can use ``rsatoolbox.inference.eval_bootstrap_pattern``.
This method will perform a bootstrap resampling of the conditions to estimate the uncertainty. This method takes the following inputs additionally
to the ones of ``eval_fixed``: ``N`` sets the number of bootstrap samples to use, ``pattern_descriptor`` is an optional argument to group patterns together.
If a name of a pattern_descriptor is passed, all patterns with an equal entry are included or excluded together.  And ``boot_noise_ceil`` switches
bootstrapping of the noise ceiling on or off. Bootstrapping the noise ceiling (``boot_noise_ceil=true``) is slightly more accurate as average performance over subsampled RDMs
can be different from overall performance, but this takes noticeably more computation time.

If we want to generalize over both subjects and conditions/stimuli, we need to apply our novel 2D bootstrap method. This method evaluates the variances
under resampling subjects and conditions both simultaneously and separately and combines these estimates into an estimate of the overall variances
of the estimates. This methods is implemented as ``rsatoolbox.inference.eval_fancy``. The only additional parameter relevant for this computation
is ``rdm_descriptor``, which allows sampling rdms together the same way ``pattern_descriptor`` allows sampling conditions together.
``eval_fancy`` also contains the methodology for performing cross-validation within the bootstrap, which requires a few more inputs
that can be ignored when all inputs are fixed models.


3. Cross-validation
-------------------
For evaluating flexible models, that allow some fitting of parameters we use cross-validation to avoid a bias towards more flexible models.
``rsatoolbox`` provides two main functions to do this: ``rsatoolbox.inference.crossval`` which performs a single crossvalidation for a given split
of the data and ``rsatoolbox.inference.eval_fancy`` that performs a crossvalidation within a bootstrap to estimate the uncertainty of this
evaluation as well.




Results objects
---------------
A results object contains all information about the analysis that requires substantial computation time. The intended use is to pass this object
directly to visualization functions, test function etc. and do not need to consult the contents directly often. They are accessible for direct access
nonetheless.

The results object contains the following information:

``cv_method``:

    a string specifying the inference method used

``diff_var``:

    variances estimates for all pairwise model differences as a 2D numpy array

``dof``:

    Degrees of freedom for t-tests. The number of levels of the smaller factor generalization is attempted over minus 1.
    For a dataset with 20 stimuli and 10 subjects this would be 9 for generalization over both or subjects only and 19 for generalization over stimuli only.

``evaluations``:

    all evaluation values computed. This is an up to 4 dimensional numpy array (boostrap samples x models x crossvalidation folds (rdm + pattern)).

``method``:

    the RDM similarity measure used for evaluation.

``model_var``:

    variance estimate for each model

``n_model``:

    the number of models evaluated

``noise_ceiling``:

    noise ceiling estimate

``noise_ceil_var``:

    variance estimate for the noise ceiling

``variances``:

    internal covariance matrix over models and the noise ceiling. Usually, ``model_var``, ``diff_var``, and ``noise_ceil_var``, which are derived
    from this matrix are meant for user access.
